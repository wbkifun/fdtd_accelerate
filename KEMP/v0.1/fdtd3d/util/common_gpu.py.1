import pyopencl as cl
import common
import sys
import os


src_path = os.path.sep.join( (os.path.dirname(__file__), 'gpu', 'src', '') )


def get_gpu_devices(print_verbose=True):
	platforms = cl.get_platforms()
	for platform in platforms:
		if print_verbose:
			print('Platform : %s (%s)' % (platform.get_info(cl.platform_info.NAME), platform.get_info(cl.platform_info.VERSION)))

	if len(platforms) > 1:
		print('Error: %d platforms are found. Sorry! We currently do not support the multi platforms.')
		sys.exit()

	platform = platforms[0]
	gpu_devices = platform.get_devices(device_type=cl.device_type.GPU)

	if print_verbose:
		print_gpu_info(gpu_devices)

	return gpu_devices



def print_gpu_info(devices):
	gpu_groups = {}
	for device in devices:
		name = device.get_info(cl.device_info.NAME)
		if not gpu_groups.has_key(name):
			gpu_groups[name] = {'count':1}
			gpu_groups[name]['compute units'] = device.get_info(cl.device_info.MAX_COMPUTE_UNITS)
			gpu_groups[name]['global mem size'] = device.get_info(cl.device_info.GLOBAL_MEM_SIZE)
			gpu_groups[name]['local mem size'] = device.get_info(cl.device_info.LOCAL_MEM_SIZE)
			gpu_groups[name]['constant mem size'] = device.get_info(cl.device_info.MAX_CONSTANT_BUFFER_SIZE)
		else:
			gpu_groups[name]['count'] += 1

	for name, props in gpu_groups.items():
		print('Device : %d GPU' % props['count'])
		print('  name: %s' % name)
		print('  compute units: %d' % props['compute units'])
		print('  global mem size: %1.2f %s' % common.get_nbytes_unit(props['global mem size']))
		print('  local mem size: %1.2f %s' % common.get_nbytes_unit(props['local mem size']))
		print('  constant mem size: %1.2f %s' % common.get_nbytes_unit(props['constant mem size']))
		print('')



def get_optimal_global_work_size(device):
	warp_size = 32
	max_resident_warp_dict = {
			'1.0':24, '1.1':24,
			'1.2':32, '1.3':32,
			'2.0':48}
	compute_capability = \
			str(device.get_info(cl.device_info.COMPUTE_CAPABILITY_MAJOR_NV)) \
			+ '.' + str(device.get_info(cl.device_info.COMPUTE_CAPABILITY_MINOR_NV))
	max_resident_warp = max_resident_warp_dict[compute_capability]
	max_compute_units = device.get_info(cl.device_info.MAX_COMPUTE_UNITS)

	return max_compute_units * max_resident_warp * warp_size




if __name__ == '__main__':
	gpu_devices = get_gpu_devices()
	print_gpu_info(gpu_devices)
	print('Optimal Gs = %d' % get_optimal_global_work_size(gpu_devices[0]))
